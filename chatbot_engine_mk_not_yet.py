# chatbot_engine.py
# Core engine: env loading, memory, topic-label injection, deictic hard-guard,
# active-context rewriting, RAG chain, formatting helpers.

from __future__ import annotations
import os, re, json, uuid
from pathlib import Path
from datetime import datetime
from functools import lru_cache
from typing import Optional, Tuple, List

import numpy as np
from dotenv import dotenv_values

from langchain_core.runnables import Runnable, RunnableMap, RunnableLambda, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.retrievers import TavilySearchAPIRetriever


# ---------- Formatting helpers (exported) ----------
def format_mcq(text: str) -> str:
    patterns = [
        (r"\s*â‘ ", r"\nâ‘ "), (r"\s*â‘¡", r"\nâ‘¡"), (r"\s*â‘¢", r"\nâ‘¢"), (r"\s*â‘£", r"\nâ‘£"),
        (r"\s*â‘¤", r"\nâ‘¤"), (r"\s*â‘¥", r"\nâ‘¥"), (r"\s*â‘¦", r"\nâ‘¦"), (r"\s*â‘§", r"\nâ‘§"),
        (r"\s*â‘¨", r"\nâ‘¨"), (r"\s*â‘©", r"\nâ‘©"),
        (r"\s*1\)", r"\nâ‘ "), (r"\s*2\)", r"\nâ‘¡"), (r"\s*3\)", r"\nâ‘¢"), (r"\s*4\)", r"\nâ‘£"),
        (r"\s*5\)", r"\nâ‘¤"), (r"\s*6\)", r"\nâ‘¥"),
    ]
    for pat, rep in patterns:
        text = re.sub(pat, rep, text)
    text = re.sub(r"\s*(Answer\s*:\s*[^\n]+)", r"\n\1", text)
    text = re.sub(r"(Answer\s*:\s*[^\n]+)\s*(?=Question\s*\d+\.)", r"\1\n\n", text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+\n", "\n", text)
    return text.strip()

def to_hard_breaks(md: str) -> str:
    md = md.replace("\r\n", "\n").replace("\r", "\n")
    # single LF -> markdown hard break "  \n"
    return re.sub(r"(?<!\n)\n(?!\n)", "  \n", md)

def get_choice_labels(n: int) -> list[str]:
    digits = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"
    n = max(2, min(n, 10))
    return list(digits[:n])


# ---------- Memory ----------
class MemoryManager:
    def __init__(self, session_id: str, base_dir: str = "./mycache/conversations"):
        self.session_id = session_id or uuid.uuid4().hex
        self.base_dir = base_dir
        os.makedirs(self.base_dir, exist_ok=True)
        self.path = Path(self.base_dir) / f"{self.session_id}.json"
        self._load()

    def _load(self):
        if self.path.exists():
            try:
                self.memory: list[dict] = json.loads(self.path.read_text(encoding="utf-8"))
            except Exception:
                self.memory = []
        else:
            self.memory = []

    def save(self):
        try:
            self.path.write_text(json.dumps(self.memory, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception:
            pass

    def reset(self):
        self.session_id = uuid.uuid4().hex
        self.path = Path(self.base_dir) / f"{self.session_id}.json"
        self.memory = []
        self.save()

    def add_turn(self, user_text: str, assistant_text: str):
        self.memory.append({
            "time": datetime.now().isoformat(timespec="seconds"),
            "user": user_text,
            "assistant": assistant_text
        })
        self.save()

    def last_user_text(self) -> Optional[str]:
        for t in reversed(self.memory):
            if t.get("user"):
                return t["user"]
        return None

    def render_capped(self, max_chars: int = 4000) -> str:
        lines = []
        for turn in self.memory:
            lines.append(f"[User @ {turn['time']}] {turn['user']}")
            lines.append(f"[Assistant] {turn['assistant']}")
        s = "\n".join(lines)
        return s[-max_chars:] if len(s) > max_chars else s

    def recent_user_texts(self, k: int = 5) -> list[str]:
        return [t.get("user", "") for t in self.memory if t.get("user")][-k:]


# ---------- Engine ----------
class ChatBotEngine:
    def __init__(
        self,
        session_id: str,
        keys_env_path: str = "/home/mk/workspace/KEYS.env",
        conversations_dir: str = "./mycache/conversations",
        embed_cache_dir: str = "./mycache/embedding",
        max_history_chars: int = 4000,
    ):
        self.keys_env_path = keys_env_path
        self.ensure_env()
        self.max_history_chars = max_history_chars

        self.memory = MemoryManager(session_id=session_id, base_dir=conversations_dir)
        os.makedirs(embed_cache_dir, exist_ok=True)
        self.embed_store = LocalFileStore(embed_cache_dir)

        # ğŸ”’ ë„ë©”ì¸ ë¼ë²¨ ìºì‹œ(ì˜ˆ: "SQLD (SQL ê°œë°œì)")
        self.cached_label: str = ""

    # ----- ENV -----
    def ensure_env(self):
        p = Path(self.keys_env_path)
        if p.exists():
            vals = dotenv_values(p)
            oa = vals.get("OPENAI_API_KEY") or vals.get("OPENAI_KEY")
            if oa: os.environ["OPENAI_API_KEY"] = oa.strip().strip('"').strip("'")
            tv = vals.get("TAVILY_API_KEY") or vals.get("TAVILY_KEY")
            if tv: os.environ["TAVILY_API_KEY"] = tv.strip().strip('"').strip("'")

    # ----- Embedder (cached) -----
    @staticmethod
    @lru_cache(maxsize=1)
    def get_embedder_cached(api_key: str):
        return OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)

    def get_embedder(self):
        return self.get_embedder_cached(os.getenv("OPENAI_API_KEY") or "")

    # ----- Deictic / meta / label -----
    DEFAULT_DEICTIC_TERMS = [
        "ì´ê²ƒ","ê·¸ê²ƒ","ì €ê²ƒ","ì´ê±°","ê·¸ê±°","ì €ê±°","ì´ê±°ìš”","ê·¸ê±°ìš”",
        "ì´ê±´","ê·¸ê±´","ì €ê±´","ì´ëŸ°","ê·¸ëŸ°","ì €ëŸ°","ì´ëŸ°ê±°","ê·¸ëŸ°ê±°","ì €ëŸ°ê±°",
        "ì—¬ê¸°","ê±°ê¸°","ì €ê¸°","ì´ìª½","ê·¸ìª½","ì €ìª½",
        "ì§€ê¸ˆ","ë°©ê¸ˆ","ì•„ê¹Œ","ë°©ê¸ˆ ì „","ì´ë•Œ","ê·¸ë•Œ","ì €ë•Œ",
        "ê·¸ ë¬¸ì„œ","ê·¸ ìë£Œ","ê·¸ ë‚´ìš©","ê·¸ ë¬¸ì œ","ì´ ë¬¸ì œ","ìœ„ ë‚´ìš©","ì•ì— ë§í•œ","ë§í•œê±°","ë§ì”€í•˜ì‹ ",
        "ê·¸ê±° ë‹¤ì‹œ","ê·¸ê±° ë­ì˜€ì§€","ê·¸ëŸ¬ë©´","ê·¸ëŸ¼","ê·¸ë ‡ë‹¤ë©´"
    ]
    _DEICTIC_RE = re.compile("|".join(map(re.escape, sorted(DEFAULT_DEICTIC_TERMS, key=len, reverse=True))))

    @staticmethod
    def adjust_topic_threshold(user_text: str, base_threshold: float) -> float:
        eff = float(base_threshold)
        t = (user_text or "").strip()
        if ChatBotEngine._DEICTIC_RE.search(t): eff -= 0.12
        if len(t) <= 10: eff -= 0.05
        return max(0.45, min(eff, 0.95))

    @staticmethod
    def _normalize_meta_text(text: str) -> str:
        if not text: return ""
        t = text.strip()
        t = re.sub(r"[ã…£l|]+$", "", t)
        t = re.sub(r"\s+", " ", t)
        return t

    _MEMORY_QUERY_RE = re.compile(
        r"(?:(?:ì „ì—|ë°©ê¸ˆ|ì•„ê¹Œ)\s*(?:ë‚´ê°€\s*)?(?:ë­|ë¬´ì—‡)(?:ë¼|ë¼ê³ )?\s*(?:ë§|ì§ˆë¬¸|ë¬¼ì–´(?:ë´¤|ë³´ì•˜)|ë¬¼ì—ˆ|í–ˆ)\w*(?:ë”ë¼|ë“œë¼|ì§€|ë‚˜ìš”|ë‹ˆ|ì˜€ë‹ˆ|ì—ˆë‹ˆ)?\??"
        r"|(?:ì´ì „|ì§ì „|ë§ˆì§€ë§‰)\s*(?:ì§ˆë¬¸|ë§|ë°œí™”)\s*(?:ì´|ì€)?\s*(?:ë­ì˜€|ë¬´ì—‡ì´ì—ˆ)\w*"
        r"|(?:what\s+did\s+i\s+(?:ask|say)\s+(?:before|earlier)|previous\s+question|last\s+question))",
        re.IGNORECASE,
    )

    def is_meta_memory_query(self, text: str) -> bool:
        t = self._normalize_meta_text(text)
        if self._MEMORY_QUERY_RE.search(t):
            return True
        # semantic fallback
        examples = [
            "ë‚´ê°€ ì „ì— ë­ë¼ ì§ˆë¬¸í–ˆë”ë¼?",
            "ì´ì „ ì§ˆë¬¸ì´ ë­ì˜€ì§€?",
            "What did I ask before?",
            "What was my previous question?",
        ]
        emb = self.get_embedder()
        v = np.array(emb.embed_query(t), dtype=float)
        vnorm = np.linalg.norm(v) + 1e-12
        sims = []
        for ex in examples:
            r = np.array(emb.embed_query(ex), dtype=float)
            sims.append((v @ r) / ((np.linalg.norm(r) + 1e-12) * vnorm))
        return max(sims) >= 0.82

    STOPWORDS_KO = {
        "ì§€ê¸ˆ","ì§ˆë¬¸","í•˜ê³ ","ìˆì–ì•„","ì¶”ì²œ","í•´ì¤˜","í•´ì£¼ì„¸ìš”","ì£¼ì„¸ìš”","ì¢€","ê´€ë ¨","ë„ì›€",
        "ì±…","êµì¬","ë¬¸ì œ","ì–´ë–»ê²Œ","ì•Œë ¤ì¤˜","ë­","ë­ì•¼","ìš”","ê²ƒ","ê·¸ê²ƒ","ì €ê²ƒ",
        "ì´ê±°","ê·¸ê±°","ì €ê±°","ì´ê±´","ê·¸ê±´","ì €ê±´","ì •ë³´","ìš”ì•½","ìë£Œ","ì„¤ëª…","ë°©ë²•","ê·¸ëŸ¬ë©´","ê·¸ëŸ¼"
    }
    SQLD_PAT_KO = re.compile(r"(SQLD|SQL\-?D|SQL\s*ê°œë°œì|ë°ì´í„°ë² ì´ìŠ¤|ì •ê·œí™”|ì¡°ì¸|ì¿¼ë¦¬|DDL|DML)", re.IGNORECASE)

    @staticmethod
    def _tokenize_ko(text: str):
        toks = re.findall(r"[A-Za-z0-9ê°€-í£]+", text or "")
        return [t for t in toks if len(t) > 1 and t not in ChatBotEngine.STOPWORDS_KO]

    def derive_topic_label_from_texts(self, texts: List[str]) -> str:
        joined = " ".join(texts[-5:])
        if self.SQLD_PAT_KO.search(joined):
            return "SQLD (SQL ê°œë°œì)"
        return ""

    GENERIC_AMBIGUOUS_TERMS = {
        "ë¬¸ì œ","ì˜ˆì œ","ì—°ìŠµ","ì„ ìˆ˜","ê°ë…","ìˆœìœ„","ë¦¬ìŠ¤íŠ¸","ì¶”ì²œ","ë°©ë²•","ì •ì˜","ì„¤ëª…",
        "ìë£Œ","ìš”ì•½","ê°•ì˜","êµì¬","ì±…","ë¬¸ì œì§‘","ê¸°ì¶œ","ìë£Œì§‘","ìˆ˜ì—…","ê°•ì¢Œ","ì½”ìŠ¤",
        "í¬ì§€ì…˜","ìœ ëª…í•œ","ì •ë³´","ë¹„êµ","íŠ¹ì§•","ê¸°ë³¸","ê°œë…"
    }

    def rewrite_with_active_context(self, user_text: str, scope_hint: str, topic_label: str) -> str:
        t = (user_text or "").strip()
        if not t:
            return user_text
        short = len(t) <= 14
        has_deictic = bool(self._DEICTIC_RE.search(t))
        has_generic = any(term in t for term in self.GENERIC_AMBIGUOUS_TERMS)
        ambiguous = short or has_deictic or has_generic
        if not ambiguous:
            return t
        label_prefix = ""
        if topic_label and topic_label.lower() not in t.lower():
            label_prefix = f"{topic_label} ê´€ë ¨ "
        tag = f"[í™œì„±ì£¼ì œ: {scope_hint}]" if scope_hint else ""
        return f"{label_prefix}{t} {tag} â€” ë²”ìœ„ë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ìœ„ í™œì„±ì£¼ì œ/ë¼ë²¨ì— í•œì •í•´ ë‹µí•˜ë¼."

    # ----- Topic detection -----
    def calc_similarity_with_last(self, user_text: str) -> Optional[float]:
        last = self.memory.last_user_text()
        if not last:
            return None
        emb = self.get_embedder()
        v_new = np.array(emb.embed_query(user_text), dtype=float)
        v_old = np.array(emb.embed_query(last), dtype=float)
        return float(v_new @ v_old) / (np.linalg.norm(v_new) * np.linalg.norm(v_old) + 1e-12)

    def should_reset_topic(
        self,
        user_text: str,
        threshold: float = 0.72,
        sticky_margin: float = 0.05
    ) -> Tuple[bool, Optional[Tuple[float, float]]]:
        # ğŸ”’ í•˜ë“œ ê°€ë“œ: ì§€ì‹œí‘œí˜„ì´ ìˆìœ¼ë©´ ì ˆëŒ€ ë¦¬ì…‹í•˜ì§€ ì•ŠìŒ
        if self._DEICTIC_RE.search(user_text or ""):
            return False, (1.0, threshold)  # debugìš© ê°’
        if self.is_meta_memory_query(user_text):
            return False, None
        last = self.memory.last_user_text()
        if not last:
            return False, None
        emb = self.get_embedder()
        v_new = np.array(emb.embed_query(user_text), dtype=float)
        v_old = np.array(emb.embed_query(last), dtype=float)
        sim = float(v_new @ v_old) / (np.linalg.norm(v_new) * np.linalg.norm(v_old) + 1e-12)
        eff = self.adjust_topic_threshold(user_text, threshold)
        return sim < (eff - sticky_margin), (sim, eff)

    # ----- Scope & label helpers -----
    def scope_hint(self, k: int = 3) -> str:
        texts = self.memory.recent_user_texts(k)
        if not texts: return ""
        hint = " / ".join(texts)
        return hint[:500]

    def topic_label(self) -> str:
        # ìµœê·¼ ë°œí™”ì—ì„œ ë¼ë²¨ì„ ë½‘ì•„ ê°±ì‹ , ì—†ìœ¼ë©´ ìºì‹œ ìœ ì§€
        derived = self.derive_topic_label_from_texts(self.memory.recent_user_texts(5))
        if derived:
            self.cached_label = derived
            return derived
        return self.cached_label

    # ì „ì²´ ì£¼ì œ ë¦¬ì…‹(ë¼ë²¨ ìºì‹œ í¬í•¨)
    def reset_topic(self):
        self.memory.reset()
        self.cached_label = ""

    # ----- RAG components -----
    class FallbackRetriever(Runnable):
        def __init__(self, primary, fallback):
            self.primary = primary
            self.fallback = fallback
        def invoke(self, input, config=None):
            try:
                results = self.primary.invoke(input, config=config)
                if results:
                    return results
            except Exception:
                pass
            return self.fallback.invoke(input, config=config)

    def _openai_embeddings(self):
        return OpenAIEmbeddings(model="text-embedding-3-small", api_key=os.getenv("OPENAI_API_KEY"))

    def make_vectorstore_from_documents(self, split_documents):
        cached = CacheBackedEmbeddings.from_bytes_store(self._openai_embeddings(), self.embed_store)
        return FAISS.from_documents(documents=split_documents, embedding=cached)

    def build_retriever_from_pdf(self, file_path: str, chunk_size: int, chunk_overlap: int):
        docs = PDFPlumberLoader(file_path).load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        vectorstore = self.make_vectorstore_from_documents(splitter.split_documents(docs))
        return vectorstore.as_retriever()

    def build_retriever_from_text(self, text: str, use_semantic: bool, chunk_size: int, chunk_overlap: int):
        if use_semantic:
            splitter = SemanticChunker(self._openai_embeddings(), breakpoint_threshold_type="percentile", breakpoint_threshold_amount=70)
            docs = splitter.create_documents([text])
        else:
            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            docs = splitter.create_documents([text])
        vectorstore = self.make_vectorstore_from_documents(docs)
        return vectorstore.as_retriever()

    def safe_tavily_retriever(self, k: int = 3):
        if os.getenv("TAVILY_API_KEY"):
            os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")
            return TavilySearchAPIRetriever(k=k)
        class EmptyRetriever(Runnable):
            def invoke(self, input, config=None): return []
        return EmptyRetriever()

    # ----- Prompt / Chain -----
    @staticmethod
    def build_prompt_text(mode: str) -> str:
        header_common = """[ì§€ì‹œ]
- í˜„ì¬ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ê³¼ê±° íˆìŠ¤í† ë¦¬ëŠ” **ë¬´ì‹œ**í•˜ë¼(ìë™ ì£¼ì œ ì „í™˜ ì‹œ ì´ì „ ì£¼ì œëŠ” ì œì™¸ë¨).
- ì•„ë˜ì˜ <chat_history>ì™€ <active_context>ë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ë¼.
- ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì¼ë°˜ëª…ì‚¬ ìœ„ì£¼ì¼ ê²½ìš°, ì‚¬ìš©ìê°€ ë²”ìœ„ë¥¼ ë°”ê¾¸ì—ˆë‹¤ê³  **ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´**
  <active_context>ì˜ ë²”ìœ„ë¥¼ **ê¸°ë³¸ê°’**ìœ¼ë¡œ ê°€ì •í•˜ë¼.
- ì§ˆë¬¸ ë¬¸ìì—´ì— **[í™œì„±ì£¼ì œ: â€¦]** íƒœê·¸ë‚˜ **'â€¦ ê´€ë ¨' ë„ë©”ì¸ ë¼ë²¨**ì´ ë³´ì´ë©´, ê·¸ ë²”ìœ„ë¥¼ **ìµœìš°ì„ ìœ¼ë¡œ ì ìš©**í•˜ë¼.
"""
        history_block = """
<chat_history>
{history}
</chat_history>

<active_context>
{active_context}
</active_context>
"""
        if mode == "ì¼ë°˜ ëŒ€í™”":
            return f"""ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

{header_common}
- ì•„ë˜ì˜ <information> ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  í™œìš©í•˜ë˜, ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë¼.
- ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ë¼.
{history_block}
<information>
{{context}}
</information>

#Question:
{{question}}
"""
        if mode == "ìš”ì•½":
            return f"""ë‹¤ìŒ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. í•„ìš”ì‹œ í•­ëª©ë³„ ë¶ˆë¦¿ì„ ì‚¬ìš©í•˜ë¼.
{header_common}
{history_block}
<information>
{{context}}
</information>

ìš”ì•½ ëŒ€ìƒ/ìš”ì²­:
{{question}}
"""
        if mode == "MCQ(ê°ê´€ì‹)":
            return f"""ë‹¹ì‹ ì€ ê°ê´€ì‹ ë¬¸í•­ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ê°ê´€ì‹ ë¬¸í•­ì„ ë§Œë“¤ì–´ë¼.
- ê° ë¬¸í•­ì€ ë³´ê¸° {{num_choices}}ê°œë¡œ êµ¬ì„±í•˜ê³ , ë³´ê¸° í‘œì‹œëŠ” ë‹¤ìŒ ë¼ë²¨ì„ **ì´ ìˆœì„œ**ë¡œ ì‚¬ìš©í•˜ë¼: {{choice_labels_text}}
- ë¬¸ì œë¬¸ ë‹¤ìŒ ì¤„ë¶€í„° ê° ë³´ê¸°ë¥¼ **í•œ ì¤„ì— í•˜ë‚˜ì”©** ì¶œë ¥í•˜ë¼.
- ë§ˆì§€ë§‰ ì¤„ì— ì •ë‹µì„ 'Answer: â‘ ' í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ(í˜•ì‹ë§Œ ì°¸ê³ )
Question 1. â€¦ë¬¸ì œë¬¸â€¦
â‘  ë³´ê¸°1
â‘¡ ë³´ê¸°2
â‘¢ ë³´ê¸°3
â‘£ ë³´ê¸°4
Answer: â‘¢
"""
        if mode == "OX(ì°¸/ê±°ì§“)":
            return f"""ë‹¹ì‹ ì€ OX(ì°¸/ê±°ì§“) ë¬¸ì œ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ì§„ìˆ ë¬¸ì„ ë§Œë“¤ê³ , ê° ë¬¸í•­ì˜ ì •ë‹µì„ 'Answer: O' ë˜ëŠ” 'Answer: X' í•œ ì¤„ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ
Q1. â€¦ì§„ìˆ ë¬¸â€¦
Answer: O

Q2. â€¦ì§„ìˆ ë¬¸â€¦
Answer: X
"""
        if mode == "ë‹¨ë‹µí˜• í€´ì¦ˆ":
            return f"""ë‹¹ì‹ ì€ ë‹¨ë‹µí˜• í€´ì¦ˆ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ë‹¨ë‹µí˜• ë¬¸í•­ì„ ë§Œë“¤ê³ , ê° ë¬¸í•­ì˜ ì •ë‹µì„ 'Answer: â€¦' í•œ ì¤„ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ
Q1. â€¦ë¬¸ì œë¬¸â€¦
Answer: â€¦

Q2. â€¦ë¬¸ì œë¬¸â€¦
Answer: â€¦
"""
        # í˜¼í•©
        return f"""ë‹¹ì‹ ì€ í˜¼í•©í˜• í€´ì¦ˆ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œ ë¬¸í•­ì„ ìƒì„±í•˜ë˜, ëŒ€ëµ 50%ëŠ” ê°ê´€ì‹, 25%ëŠ” OX, 25%ëŠ” ë‹¨ë‹µí˜•ìœ¼ë¡œ ì„ì–´ë¼.
- ê°ê´€ì‹ì˜ ë³´ê¸° ìˆ˜ëŠ” {{num_choices}}ê°œì´ë©°, ë³´ê¸° ë¼ë²¨ì€ ë‹¤ìŒì„ ì‚¬ìš©: {{choice_labels_text}}
- ê° ìœ í˜•ì˜ ì¶œë ¥ í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œë¼:
  - ê°ê´€ì‹: ë¬¸ì œë¬¸ -> ê° ë³´ê¸°(í•œ ì¤„ì”©) -> 'Answer: â‘ ' í˜•íƒœ
  - OX: 'Qn. â€¦' -> 'Answer: O/X'
  - ë‹¨ë‹µí˜•: 'Qn. â€¦' -> 'Answer: â€¦'
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}
"""

    def create_chain(self, retriever, mode: str, num_q: int, num_choices: int, choice_labels_text: str):
        tavily = self.safe_tavily_retriever(k=3)
        fallback = ChatBotEngine.FallbackRetriever(retriever, tavily)
        prompt = ChatPromptTemplate.from_template(self.build_prompt_text(mode))
        llm = ChatOpenAI(model="gpt-4.1", temperature=0, api_key=os.getenv("OPENAI_API_KEY"))
        return (
            RunnableMap({
                "context": fallback | RunnableLambda(lambda x: "\n".join(
                    getattr(d, "page_content", str(d)) for d in (x or [])
                )),
                "question": RunnablePassthrough(),
                "history": RunnableLambda(lambda _: self.memory.render_capped(self.max_history_chars)),
                "active_context": RunnableLambda(lambda _: self.scope_hint()),
                "num_q": RunnableLambda(lambda _: num_q),
                "num_choices": RunnableLambda(lambda _: num_choices),
                "choice_labels_text": RunnableLambda(lambda _: choice_labels_text),
            })
            | prompt
            | llm
            | StrOutputParser()
        )
