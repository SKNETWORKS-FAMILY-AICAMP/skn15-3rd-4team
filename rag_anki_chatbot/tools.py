# tools.py
import json
import requests
from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
import time
import chardet
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

ANKI_CONNECT_URL = "http://192.168.160.1:8765"
tavily_search_tool = TavilySearchResults(max_results=3)

# ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
VECTORSTORE_DSN = "postgresql+psycopg2://play:123@localhost:5432/play"
DOCUMENT_COLLECTION_NAME = "play_documents"

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
document_vector_store = PGVector(
    connection=VECTORSTORE_DSN,
    embeddings=embeddings,
    collection_name=DOCUMENT_COLLECTION_NAME,
    use_jsonb=True,
)

def web_search(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ ë„êµ¬"""
    try:
        print(f"ğŸ–¥ï¸  ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query}")
        results = tavily_search_tool.invoke({"query": query})
        return f"'{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n{results}"
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def document_search(query: str) -> str:
    """
    ğŸ”§ ë””ë²„ê¹…ì´ ê°•í™”ëœ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
    - ì ìˆ˜ ê³„ì‚° ê³¼ì • ìƒì„¸ ë¡œê¹…
    - ì„ê³„ê°’ ë¬¸ì œ í•´ê²°
    - ë” ê´€ëŒ€í•œ í•„í„°ë§
    """
    try:
        print(f"ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰: '{query}'")
        
        # 1. ì´ˆê¸° ê²€ìƒ‰
        retriever = document_vector_store.as_retriever(search_kwargs={"k": 15})
        documents = retriever.invoke(query)
        
        if not documents:
            print("   âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŒ")
            return "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        print(f"   ğŸ” ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # 2. ğŸ”¥ ë””ë²„ê¹…: ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        for i, doc in enumerate(documents[:3], 1):
            preview = doc.page_content[:100].replace('\n', ' ')
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            print(f"   ğŸ“‹ ë¬¸ì„œ {i}: {source} - {preview}...")
        
        # 3. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë””ë²„ê¹… ê°•í™”)
        scored_docs = []
        query_embedding = embeddings.embed_query(query)
        print(f"   ğŸ§® ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")
        
        for i, doc in enumerate(documents):
            try:
                # ë¬¸ì„œ ì„ë² ë”© ê³„ì‚°
                doc_embedding = embeddings.embed_query(doc.page_content)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    [query_embedding], [doc_embedding]
                )[0][0]
                
                # ğŸ”¥ ë””ë²„ê¹…: ì ìˆ˜ ì¶œë ¥
                source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                chunk_id = doc.metadata.get('chunk_id', 0)
                print(f"   ğŸ“Š ë¬¸ì„œ {i+1} ì ìˆ˜: {similarity:.4f} ({source}, ì²­í¬ {chunk_id})")
                
                scored_docs.append({
                    'document': doc,
                    'score': similarity,
                    'source': source,
                    'chunk_id': chunk_id
                })
                
            except Exception as e:
                print(f"   âŒ ë¬¸ì„œ {i+1} ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        if not scored_docs:
            return "ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        # 4. ì ìˆ˜ ì •ë ¬
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        # ğŸ”¥ ì ìˆ˜ ë¶„í¬ ë¶„ì„
        scores = [doc['score'] for doc in scored_docs]
        max_score = max(scores) if scores else 0
        min_score = min(scores) if scores else 0
        avg_score = sum(scores) / len(scores) if scores else 0
        
        print(f"   ğŸ“ˆ ì ìˆ˜ ë¶„í¬: ìµœê³  {max_score:.4f}, ìµœì € {min_score:.4f}, í‰ê·  {avg_score:.4f}")
        
        # 5. ğŸ”¥ ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì„¤ì •
        # ê¸°ì¡´ì˜ 0.7, 0.5ëŠ” ë„ˆë¬´ ë†’ì•˜ìŒ
        PRIMARY_THRESHOLD = 0.3    # 0.7 â†’ 0.3
        FALLBACK_THRESHOLD = 0.1   # 0.5 â†’ 0.1
        
        relevant_docs = [doc for doc in scored_docs if doc['score'] >= PRIMARY_THRESHOLD]
        print(f"   âœ… 1ì°¨ í•„í„°ë§ (ì„ê³„ê°’ {PRIMARY_THRESHOLD}): {len(relevant_docs)}ê°œ ë¬¸ì„œ")
        
        # ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì„ê³„ê°’ì„ ë” ë‚®ì¶¤
        if not relevant_docs:
            relevant_docs = [doc for doc in scored_docs if doc['score'] >= FALLBACK_THRESHOLD]
            print(f"   ğŸ”„ 2ì°¨ í•„í„°ë§ (ì„ê³„ê°’ {FALLBACK_THRESHOLD}): {len(relevant_docs)}ê°œ ë¬¸ì„œ")
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ìƒìœ„ 5ê°œë¼ë„ ë³´ì—¬ì£¼ê¸°
        if not relevant_docs:
            relevant_docs = scored_docs[:5]
            print(f"   ğŸ†˜ ê°•ì œ ì„ íƒ: ìƒìœ„ {len(relevant_docs)}ê°œ ë¬¸ì„œ (ì ìˆ˜ ë¬´ê´€)")
        
        # 6. ì¤‘ë³µ ì œê±° (ê°„ì†Œí™”)
        unique_docs = remove_duplicate_content(relevant_docs)
        print(f"   ğŸ”„ ì¤‘ë³µ ì œê±° í›„: {len(unique_docs)}ê°œ ë¬¸ì„œ")
        
        # 7. ìƒìœ„ ê²°ê³¼ë§Œ ì„ íƒ
        top_docs = unique_docs[:5]
        
        # 8. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc_info in enumerate(top_docs, 1):
            doc = doc_info['document']
            score = doc_info['score']
            source = doc_info['source']
            content = doc.page_content.strip()
            
            # ğŸ”¥ ì ìˆ˜ í‘œì‹œë¥¼ ë” ì¹œí™”ì ìœ¼ë¡œ
            score_desc = "ë†’ìŒ" if score >= 0.5 else "ì¤‘ê°„" if score >= 0.3 else "ë‚®ìŒ"
            
            context_part = f"""ğŸ“‹ **ë¬¸ì„œ {i}** (ê´€ë ¨ì„±: {score_desc}) - {source}
{content}

"""
            context_parts.append(context_part)
        
        # ìµœì¢… ê²°ê³¼
        if context_parts:
            context = f"""ğŸ” **'{query}' ê²€ìƒ‰ ê²°ê³¼** ({len(context_parts)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬)

{"".join(context_parts)}
ğŸ“Œ **ê²€ìƒ‰ ì •ë³´**: {len(documents)}ê°œ ë¬¸ì„œ ì¤‘ {len(context_parts)}ê°œ ë¬¸ì„œë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.
ğŸ“Š **ì ìˆ˜ ë²”ìœ„**: {max_score:.3f} ~ {min_score:.3f} (í‰ê·  {avg_score:.3f})"""
            
            print(f"   ğŸ“Š ìµœì¢… ì»¨í…ìŠ¤íŠ¸: {len(context)}ì, {len(context_parts)}ê°œ ë¬¸ì„œ í¬í•¨")
            return context
        else:
            return f"'{query}'ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´
        return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ğŸ”¥ ê°„ì†Œí™”ëœ ì¤‘ë³µ ì œê±° í•¨ìˆ˜
def remove_duplicate_content(scored_docs: list) -> list:
    """ì¤‘ë³µ ì œê±°ë¥¼ ë” ê°„ë‹¨í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬"""
    if len(scored_docs) <= 1:
        return scored_docs
    
    unique_docs = []
    seen_contents = set()
    
    for doc_info in scored_docs:
        content = doc_info['document'].page_content
        # ë‚´ìš©ì˜ ì²« 100ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ë” ê°„ë‹¨í•œ ë°©ì‹)
        content_key = content[:100].strip().lower()
        
        if content_key not in seen_contents:
            unique_docs.append(doc_info)
            seen_contents.add(content_key)
    
    return unique_docs

# ğŸ”¥ ì¶”ê°€: ë¬¸ì„œ í˜„í™© í™•ì¸ í•¨ìˆ˜
def check_document_status() -> str:
    """ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™©ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰
        retriever = document_vector_store.as_retriever(search_kwargs={"k": 100})
        all_docs = retriever.invoke("*")
        
        if not all_docs:
            return "âŒ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¬¸ì„œë³„ í†µê³„
        sources = {}
        for doc in all_docs:
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            if source not in sources:
                sources[source] = 0
            sources[source] += 1
        
        status_msg = f"ğŸ“Š **ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™©** (ì´ {len(all_docs)}ê°œ ì²­í¬)\n\n"
        
        for source, count in sources.items():
            status_msg += f"ğŸ“„ {source}: {count}ê°œ ì²­í¬\n"
        
        # ìƒ˜í”Œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        status_msg += f"\n**ìƒ˜í”Œ ë‚´ìš©:**\n"
        for i, doc in enumerate(all_docs[:3], 1):
            preview = doc.page_content[:50].replace('\n', ' ')
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            status_msg += f"{i}. [{source}] {preview}...\n"
        
        return status_msg
        
    except Exception as e:
        return f"âŒ ë¬¸ì„œ í˜„í™© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"

def jaccard_similarity(text1: str, text2: str) -> float:
    """
    ë‘ í…ìŠ¤íŠ¸ì˜ Jaccard ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    # Jaccard similarity = |A âˆ© B| / |A âˆª B|
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    if not union:
        return 0.0
    
    return len(intersection) / len(union)

def upload_document(file_path: str, file_name: str) -> str:
    """
    ğŸ”¥ ê°œì„ ëœ ë¬¸ì„œ ì—…ë¡œë“œ - ë” ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ì •í™•ë„ í–¥ìƒ
    """
    try:
        print(f"ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘: {file_name}")
        
        # ì¸ì½”ë”© ê°ì§€ ë° íŒŒì¼ ì½ê¸°
        with open(file_path, 'rb') as f:
            raw_data = f.read()
        encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'
        content = raw_data.decode(encoding, errors='replace')
        
        # NULL ë¬¸ì ì œê±°
        content = content.replace('\x00', '')

        # ğŸ”¥ ë” ì‘ì€ ì²­í¬ í¬ê¸°ë¡œ ë³€ê²½ (ì •í™•ë„ í–¥ìƒ)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,        # 1000 â†’ 600ìœ¼ë¡œ ì¶•ì†Œ
            chunk_overlap=150,     # 200 â†’ 150ìœ¼ë¡œ ì¡°ì •
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        chunks = text_splitter.split_text(content)
        metadatas = [{"source": file_name, "chunk_id": i} for i in range(len(chunks))]
        
        print(f"   ğŸ“„ ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨ (ì²­í¬ í¬ê¸°: 600ì)")
        
        # ë°°ì¹˜ ì²˜ë¦¬

        document_vector_store.add_texts(
            texts=chunks,
            metadatas=metadatas,
            collection_name=DOCUMENT_COLLECTION_NAME
        )
            
         

        success_msg = f"""âœ… **'{file_name}' ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ!**"""
        
        return success_msg

    except Exception as e:
        return f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def anki_card_saver(front: str, back: str, deck: str = "ê¸°ë³¸", tags: list = None) -> str:
    """
    AnkiConnect APIë¥¼ ì‚¬ìš©í•˜ì—¬ Anki ë°ìŠ¤í¬í†± í”„ë¡œê·¸ë¨ì— ìƒˆ ì¹´ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print(f"ğŸƒ Anki ì¹´ë“œ ì €ì¥ ì‹œë„: {front[:30]}...")
    
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}

    try:
        # 1. ì¤‘ë³µ ì¹´ë“œ í™•ì¸ - ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ ì•ë©´ ë‚´ìš©ì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        front_keywords = front.replace('\n', ' ')[:50]  # ì•ë©´ì˜ ì²« 50ìë¡œ ì¤‘ë³µ ê²€ì‚¬
        
        query = f'deck:"{deck}" front:*{front_keywords}*'
        find_payload = anki_request('findNotes', query=query)
        response = requests.post(ANKI_CONNECT_URL, json=find_payload)
        response.raise_for_status()
        
        print(f"   -> Anki 'findNotes' ì‘ë‹µ: {response.json()}")
        
        existing_notes = response.json().get('result', [])
        if existing_notes:
            message = f"ìœ ì‚¬í•œ ì¹´ë“œê°€ '{deck}' ë±ì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            print(f"   -> {message}")
            return message

        # 2. ìƒˆ ë…¸íŠ¸(ì¹´ë“œ) ì¶”ê°€
        note_params = {
            'note': {
                'deckName': deck,
                'modelName': 'Basic',
                'fields': {
                    'Front': front,
                    'Back': back.replace("\n", "<br>")  # HTML ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                },
                'tags': tags if tags else []
            }
        }
        
        add_payload = anki_request('addNote', **note_params)
        response = requests.post(ANKI_CONNECT_URL, json=add_payload)
        response.raise_for_status()

        print(f"   -> Anki 'addNote' ì‘ë‹µ: {response.json()}")

        response_data = response.json()
        if error := response_data.get('error'):
            raise Exception(f"AnkiConnect ì˜¤ë¥˜: {error}")
        
        note_id = response_data.get('result')
        message = f"âœ… Anki ì¹´ë“œë¥¼ '{deck}' ë±ì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (ID: {note_id})"
        print(f"   -> {message}")
        return message

    except requests.exceptions.ConnectionError:
        error_msg = "âŒ AnkiConnectì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Anki í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ ì¤‘ì´ê³  AnkiConnect ì• ë“œì˜¨ì´ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        print(f"   -> {error_msg}")
        return error_msg
    except Exception as e:
        error_msg = f"âŒ Anki ì¹´ë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"   -> {error_msg}")
        return error_msg

def list_anki_decks() -> str:
    """Ankiì˜ ëª¨ë“  ë± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"""
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}
    
    try:
        payload = anki_request('deckNames')
        response = requests.post(ANKI_CONNECT_URL, json=payload)
        response.raise_for_status()
        
        decks = response.json().get('result', [])
        if decks:
            return f"ì‚¬ìš© ê°€ëŠ¥í•œ Anki ë± ëª©ë¡:\n" + "\n".join([f"- {deck}" for deck in decks])
        else:
            return "ì‚¬ìš© ê°€ëŠ¥í•œ ë±ì´ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë± ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"