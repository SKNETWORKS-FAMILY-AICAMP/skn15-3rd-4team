# gptê¸°ë°˜ ì±—ë´‡ ëŒ€í™”ì£¼ì œ ë³€ê²½ì— ëŒ€í•œ ì •ì˜ê°€ í™•ì‹¤í•˜ì§€ ì•Šì€ ìƒíƒœ
# ì§ˆë¬¸ì„ í•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ í–ˆì„ë•Œ ëª…ì‚¬ê°€ ì—†ëŠ” ê²½ìš° ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
# ì§€ì‹œëŒ€ëª…ì‚¬ê°€ ìˆìœ¼ë©´ ëª…ì‚¬ë¥¼ ì•ì— ì•ˆë¶™í˜€ë„ ì´ì „ ì§ˆë¬¸ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë‹µ
# ê²½ë¡œ ìˆ˜ì •í•´ì•¼í•¨ 

# rag_chatbot_mk.py â€” Multi-mode + Auto Topic Shift (Deictic-aware) + Active Context + Topic Label Injection + Meta-memory + RAG + MCQ line-break fix
import os
import re
import json
import uuid
import numpy as np
from datetime import datetime
from pathlib import Path

import streamlit as st
from dotenv import dotenv_values

from langchain_core.runnables import Runnable, RunnableMap, RunnableLambda, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.retrievers import TavilySearchAPIRetriever

# =========================
# 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë”© (KEYS.env ê³ ì •)
# =========================
ENV_PATH = Path("/home/mk/workspace/KEYS.env")
if ENV_PATH.exists():
    vals = dotenv_values(ENV_PATH)
    oa = vals.get("OPENAI_API_KEY") or vals.get("OPENAI_KEY")
    if oa:
        os.environ["OPENAI_API_KEY"] = oa.strip().strip('"').strip("'")
    tv = vals.get("TAVILY_API_KEY") or vals.get("TAVILY_KEY")
    if tv:
        os.environ["TAVILY_API_KEY"] = tv.strip().strip('"').strip("'")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

MAX_HISTORY_CHARS = 4000
STICKY_MARGIN = 0.05  # í† í”½ ìœ ì§€ ì—¬ìœ 

# =========================
# 1) Streamlit ì„¤ì • + í™”ë©´ ë¡œê·¸
# =========================
st.set_page_config(page_title="RAG ê¸°ë°˜ ë©€í‹°ëª¨ë“œ ì±—ë´‡", layout="wide")
st.title("RAG ê¸°ë°˜ ë©€í‹°ëª¨ë“œ ì±—ë´‡")

if "messages" not in st.session_state:
    st.session_state.messages = []

st.markdown(
    '<div id="chatlog" style="max-height:72vh; overflow-y:auto; padding-right:8px;">',
    unsafe_allow_html=True,
)
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
st.markdown("</div>", unsafe_allow_html=True)

# =========================
# 2) ìœ í‹¸: MCQ í¬ë§· ë³´ì • + í•˜ë“œ ë¸Œë ˆì´í¬
# =========================
def format_mcq(text: str) -> str:
    patterns = [
        (r'\s*â‘ ', r'\nâ‘ '), (r'\s*â‘¡', r'\nâ‘¡'), (r'\s*â‘¢', r'\nâ‘¢'), (r'\s*â‘£', r'\nâ‘£'),
        (r'\s*â‘¤', r'\nâ‘¤'), (r'\s*â‘¥', r'\nâ‘¥'), (r'\s*â‘¦', r'\nâ‘¦'), (r'\s*â‘§', r'\nâ‘§'),
        (r'\s*â‘¨', r'\nâ‘¨'), (r'\s*â‘©', r'\nâ‘©'),
        (r'\s*1\)', r'\nâ‘ '), (r'\s*2\)', r'\nâ‘¡'), (r'\s*3\)', r'\nâ‘¢'), (r'\s*4\)', r'\nâ‘£'),
        (r'\s*5\)', r'\nâ‘¤'), (r'\s*6\)', r'\nâ‘¥'),
    ]
    for pat, rep in patterns:
        text = re.sub(pat, rep, text)
    text = re.sub(r'\s*(Answer\s*:\s*[^\n]+)', r'\n\1', text)
    text = re.sub(r'(Answer\s*:\s*[^\n]+)\s*(?=Question\s*\d+\.)', r'\1\n\n', text)
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    text = re.sub(r'[ \t]+\n', '\n', text)
    return text.strip()

def to_hard_breaks(md: str) -> str:
    md = md.replace('\r\n', '\n').replace('\r', '\n')
    return re.sub(r'(?<!\n)\n(?!\n)', '  \n', md)

def get_choice_labels(n: int):
    digits = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"
    n = max(2, min(n, 10))
    return list(digits[:n])

# =========================
# 3) ì„ë² ë”© (ìºì‹œ)
# =========================
@st.cache_resource
def get_embedder():
    return OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)

# =========================
# 4) ì§€ì‹œ/ë©”íƒ€/ëª¨í˜¸ â†’ í™œì„±ë§¥ë½ + ë„ë©”ì¸ ë¼ë²¨ ì£¼ì…
# =========================
DEFAULT_DEICTIC_TERMS = [
    "ì´ê²ƒ","ê·¸ê²ƒ","ì €ê²ƒ","ì´ê±°","ê·¸ê±°","ì €ê±°","ì´ê±°ìš”","ê·¸ê±°ìš”",
    "ì´ê±´","ê·¸ê±´","ì €ê±´","ì´ëŸ°","ê·¸ëŸ°","ì €ëŸ°","ì´ëŸ°ê±°","ê·¸ëŸ°ê±°","ì €ëŸ°ê±°",
    "ì—¬ê¸°","ê±°ê¸°","ì €ê¸°","ì´ìª½","ê·¸ìª½","ì €ìª½",
    "ì§€ê¸ˆ","ë°©ê¸ˆ","ì•„ê¹Œ","ë°©ê¸ˆ ì „","ì´ë•Œ","ê·¸ë•Œ","ì €ë•Œ",
    "ê·¸ ë¬¸ì„œ","ê·¸ ìë£Œ","ê·¸ ë‚´ìš©","ê·¸ ë¬¸ì œ","ì´ ë¬¸ì œ","ìœ„ ë‚´ìš©","ì•ì— ë§í•œ","ë§í•œê±°","ë§ì”€í•˜ì‹ ",
    "ê·¸ê±° ë‹¤ì‹œ","ê·¸ê±° ë­ì˜€ì§€","ê·¸ëŸ¬ë©´","ê·¸ëŸ¼","ê·¸ë ‡ë‹¤ë©´"
]

def build_deictic_regex(extra_terms=None):
    terms = set(DEFAULT_DEICTIC_TERMS)
    if extra_terms:
        terms.update(t.strip() for t in extra_terms if t and t.strip())
    pat = r"(" + "|".join(map(re.escape, sorted(terms, key=len, reverse=True))) + r")"
    return re.compile(pat)

_EXTRA_ENV = os.getenv("DEICTIC_EXTRA", "")
_extra_from_env = [s for s in _EXTRA_ENV.split(",")] if _EXTRA_ENV else []
_extra_from_file = []
cfg_path = Path("./config/deictic_terms.txt")
if cfg_path.exists():
    try:
        _extra_from_file = [
            line.strip() for line in cfg_path.read_text(encoding="utf-8").splitlines()
            if line.strip() and not line.strip().startswith("#")
        ]
    except Exception:
        _extra_from_file = []

_DEICTIC_RE = build_deictic_regex(_extra_from_env + _extra_from_file)

def adjust_topic_threshold(user_text: str, base_threshold: float) -> float:
    eff = float(base_threshold)
    t = (user_text or "").strip()
    if _DEICTIC_RE.search(t): eff -= 0.12
    if len(t) <= 10:          eff -= 0.05
    return max(0.45, min(eff, 0.95))

def _normalize_meta_text(text: str) -> str:
    if not text: return ""
    t = text.strip()
    t = re.sub(r"[ã…£l|]+$", "", t)
    t = re.sub(r"\s+", " ", t)
    return t

_MEMORY_QUERY_RE = re.compile(
    r"(?:"
    r"(?:ì „ì—|ë°©ê¸ˆ|ì•„ê¹Œ)\s*(?:ë‚´ê°€\s*)?(?:ë­|ë¬´ì—‡)(?:ë¼|ë¼ê³ )?\s*(?:ë§|ì§ˆë¬¸|ë¬¼ì–´(?:ë´¤|ë³´ì•˜)|ë¬¼ì—ˆ|í–ˆ)\w*(?:ë”ë¼|ë“œë¼|ì§€|ë‚˜ìš”|ë‹ˆ|ì˜€ë‹ˆ|ì—ˆë‹ˆ)?\??"
    r"|(?:ì´ì „|ì§ì „|ë§ˆì§€ë§‰)\s*(?:ì§ˆë¬¸|ë§|ë°œí™”)\s*(?:ì´|ì€)?\s*(?:ë­ì˜€|ë¬´ì—‡ì´ì—ˆ)\w*"
    r"|(?:what\s+did\s+i\s+(?:ask|say)\s+(?:before|earlier)|previous\s+question|last\s+question)"
    r")",
    re.IGNORECASE,
)

def _regex_is_meta_query(text: str) -> bool:
    return bool(_MEMORY_QUERY_RE.search(_normalize_meta_text(text)))

@st.cache_resource
def _meta_intent_refs():
    examples = [
        "ë‚´ê°€ ì „ì— ë­ë¼ ì§ˆë¬¸í–ˆë”ë¼?",
        "ì „ì— ë‚´ê°€ ë­ë¼ê³  ë§í–ˆì§€?",
        "ì´ì „ ì§ˆë¬¸ì´ ë­ì˜€ì§€?",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ì§ˆë¬¸ ì•Œë ¤ì¤˜",
        "ë°©ê¸ˆ ë‚´ê°€ ë­ë¼ í–ˆì§€",
        "What did I ask before?",
        "What did I say earlier?",
        "What was my previous question?",
        "Show my last question",
    ]
    emb = get_embedder()
    vecs = [np.array(emb.embed_query(x), dtype=float) for x in examples]
    norms = [np.linalg.norm(v) + 1e-12 for v in vecs]
    return vecs, norms

def _semantic_is_meta_query(text: str, th: float = 0.82) -> bool:
    t = _normalize_meta_text(text)
    if not t: return False
    v = np.array(get_embedder().embed_query(t), dtype=float)
    vnorm = np.linalg.norm(v) + 1e-12
    refs, norms = _meta_intent_refs()
    sims = [(v @ refs[i]) / (vnorm * norms[i]) for i in range(len(refs))]
    return max(sims) >= th

def is_meta_memory_query(text: str) -> bool:
    t = _normalize_meta_text(text)
    return _regex_is_meta_query(t) or _semantic_is_meta_query(t)

# --- Domain label extraction (lightweight) ---
STOPWORDS_KO = {
    "ì§€ê¸ˆ","ì§ˆë¬¸","í•˜ê³ ","ìˆì–ì•„","ì¶”ì²œ","í•´ì¤˜","í•´ì£¼ì„¸ìš”","ì£¼ì„¸ìš”","ì¢€","ê´€ë ¨","ë„ì›€",
    "ì±…","êµì¬","ë¬¸ì œ","ì–´ë–»ê²Œ","ì•Œë ¤ì¤˜","ë­","ë­ì•¼","ìš”","ê²ƒ","ê·¸ê²ƒ","ì €ê²ƒ",
    "ì´ê±°","ê·¸ê±°","ì €ê±°","ì´ê±´","ê·¸ê±´","ì €ê±´","ì •ë³´","ìš”ì•½","ìë£Œ","ì„¤ëª…","ë°©ë²•","ê·¸ëŸ¬ë©´","ê·¸ëŸ¼"
}
SQLD_PAT_KO = re.compile(r"(SQLD|SQL\-?D|SQL\s*ê°œë°œì|ë°ì´í„°ë² ì´ìŠ¤|ì •ê·œí™”|ì¡°ì¸|ì¿¼ë¦¬|DDL|DML)", re.IGNORECASE)

def _tokenize_ko(text: str):
    toks = re.findall(r"[A-Za-z0-9ê°€-í£]+", text or "")
    return [t for t in toks if len(t) > 1 and t not in STOPWORDS_KO]

def derive_topic_label_from_texts(texts: list[str]) -> str:
    joined = " ".join(texts[-5:])  # ìµœê·¼ ìµœëŒ€ 5ê°œ
    if SQLD_PAT_KO.search(joined):
        return "SQLD (SQL ê°œë°œì)"
    return ""

# ëª¨í˜¸ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (í’ë¶€)
GENERIC_AMBIGUOUS_TERMS = {
    "ë¬¸ì œ","ì˜ˆì œ","ì—°ìŠµ","ì„ ìˆ˜","ê°ë…","ìˆœìœ„","ë¦¬ìŠ¤íŠ¸","ì¶”ì²œ","ë°©ë²•","ì •ì˜","ì„¤ëª…",
    "ìë£Œ","ìš”ì•½","ê°•ì˜","êµì¬","ì±…","ë¬¸ì œì§‘","ê¸°ì¶œ","ìë£Œì§‘","ìˆ˜ì—…","ê°•ì¢Œ","ì½”ìŠ¤",
    "í¬ì§€ì…˜","ìœ ëª…í•œ","ì •ë³´","ë¹„êµ","íŠ¹ì§•","ê¸°ë³¸","ê°œë…"
}

def rewrite_with_active_context(user_text: str, scope_hint: str, topic_label: str) -> str:
    """
    ì§ˆë¬¸ì´ ì§§ê±°ë‚˜ ì§€ì‹œí‘œí˜„/ì¼ë°˜ëª…ì‚¬ ìœ„ì£¼ë¡œ ëª¨í˜¸í•˜ë©´
    1) [í™œì„±ì£¼ì œ: ...] íƒœê·¸
    2) topic_label(ì˜ˆ: 'SQLD (SQL ê°œë°œì)')ì„ ì§ˆë¬¸ì— ì§ì ‘ ì£¼ì…í•˜ì—¬ í•´ì„ì„ ê°•ì œ.
    """
    t = (user_text or "").strip()
    if not t:
        return user_text

    short = len(t) <= 14
    has_deictic = bool(_DEICTIC_RE.search(t))
    has_generic = any(term in t for term in GENERIC_AMBIGUOUS_TERMS)
    ambiguous = short or has_deictic or has_generic

    if not ambiguous:
        return t  # ëª¨í˜¸í•˜ì§€ ì•Šìœ¼ë©´ ì›ë¬¸ ìœ ì§€

    label_prefix = ""
    if topic_label and topic_label.lower() not in t.lower():
        label_prefix = f"{topic_label} ê´€ë ¨ "

    tag = f"[í™œì„±ì£¼ì œ: {scope_hint}]" if scope_hint else ""
    return f"{label_prefix}{t} {tag} â€” ë²”ìœ„ë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ìœ„ í™œì„±ì£¼ì œ/ë¼ë²¨ì— í•œì •í•´ ë‹µí•˜ë¼."

# =========================
# 5) ëŒ€í™” ë©”ëª¨ë¦¬ + ìë™ ì£¼ì œ ì „í™˜ (+ í™œì„± ë§¥ë½/ë¼ë²¨)
# =========================
class MemoryManager:
    def __init__(self, base_dir: str = "./mycache/conversations"):
        os.makedirs(base_dir, exist_ok=True)
        if "session_id" not in st.session_state:
            st.session_state["session_id"] = uuid.uuid4().hex
        self.base_dir = base_dir
        self.path = Path(base_dir) / f"{st.session_state['session_id']}.json"
        self._load()

    def _load(self):
        if self.path.exists():
            try:
                self.memory = json.loads(self.path.read_text(encoding="utf-8"))
            except Exception:
                self.memory = []
        else:
            self.memory = []

    def save(self):
        try:
            self.path.write_text(json.dumps(self.memory, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception as e:
            st.warning(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def reset(self):
        st.session_state["session_id"] = uuid.uuid4().hex
        self.path = Path(self.base_dir) / f"{st.session_state['session_id']}.json"
        self.memory = []
        self.save()

    def add_turn(self, user_text: str, assistant_text: str):
        self.memory.append({
            "time": datetime.now().isoformat(timespec="seconds"),
            "user": user_text,
            "assistant": assistant_text
        })
        self.save()

    def last_user_text(self):
        for t in reversed(self.memory):
            if t.get("user"):
                return t["user"]
        return None

    def calc_similarity(self, user_text: str):
        last = self.last_user_text()
        if not last:
            return None
        emb = get_embedder()
        v_new = np.array(emb.embed_query(user_text), dtype=float)
        v_old = np.array(emb.embed_query(last), dtype=float)
        return float(v_new @ v_old) / (np.linalg.norm(v_new) * np.linalg.norm(v_old) + 1e-12)

    def scope_hint(self, k: int = 3) -> str:
        texts = [t.get("user", "") for t in self.memory if t.get("user")]
        if not texts: return ""
        hint = " / ".join(texts[-k:])
        return hint[:500]

    def topic_label(self) -> str:
        texts = [t.get("user", "") for t in self.memory if t.get("user")]
        if not texts: return ""
        return derive_topic_label_from_texts(texts)

    def is_new_topic(self, user_text: str, threshold: float = 0.75) -> bool:
        if is_meta_memory_query(user_text):
            return False
        last = self.last_user_text()
        if not last:
            return False
        emb = get_embedder()
        v_new = np.array(emb.embed_query(user_text), dtype=float)
        v_old = np.array(emb.embed_query(last), dtype=float)
        sim = float(v_new @ v_old) / (np.linalg.norm(v_new) * np.linalg.norm(v_old) + 1e-12)
        eff_threshold = adjust_topic_threshold(user_text, threshold)
        st.session_state["__topic_debug"] = (sim, eff_threshold)
        return sim < (eff_threshold - STICKY_MARGIN)

    def render_capped(self, max_chars: int = MAX_HISTORY_CHARS) -> str:
        lines = []
        for turn in self.memory:
            lines.append(f"[User @ {turn['time']}] {turn['user']}")
            lines.append(f"[Assistant] {turn['assistant']}")
        s = "\n".join(lines)
        return s[-max_chars:] if len(s) > max_chars else s

memory = MemoryManager()

# =========================
# 6) ì‚¬ì´ë“œë°” UI
# =========================
with st.sidebar:
    def mask(s): return (s[:4] + "..." + s[-4:]) if s else "None"
    st.markdown("### ğŸ”‘ Keys")
    st.write("OpenAI:", bool(OPENAI_API_KEY), mask(OPENAI_API_KEY or ""))
    st.write("Tavily:", bool(TAVILY_API_KEY), mask(TAVILY_API_KEY or ""))

    st.markdown("### ğŸ§  Memory")
    use_memory = st.checkbox("ì´ì „ ëŒ€í™” ê¸°ì–µ ì‚¬ìš©", value=True)
    auto_topic = st.checkbox("ìë™ ì£¼ì œ ì „í™˜ ê°ì§€", value=True)
    topic_threshold = st.slider("ì£¼ì œ ì „í™˜ ì„ê³„ê°’(ìœ ì‚¬ë„)", 0.00, 1.00, 0.72, 0.01)
    topic_debug = st.checkbox("í† í”½ ë””ë²„ê·¸ í‘œì‹œ", value=False)
    context_debug = st.checkbox("ë§¥ë½ ë””ë²„ê·¸ í‘œì‹œ", value=False)
    if st.button("ë©”ëª¨ë¦¬ ìˆ˜ë™ ì´ˆê¸°í™”"):
        memory.reset()
        st.success("ë©”ëª¨ë¦¬ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
    if st.button("ğŸ§¹ ëŒ€í™” ëª¨ë‘ ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.success("í™”ë©´ ì±„íŒ… ë¡œê·¸ë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤. (ë©”ëª¨ë¦¬ëŠ” ê·¸ëŒ€ë¡œ)")

    st.markdown("### ğŸ§© ì‘ë‹µ ëª¨ë“œ")
    response_mode = st.selectbox(
        "ì›í•˜ëŠ” ì¶œë ¥ í˜•íƒœ",
        ["ì¼ë°˜ ëŒ€í™”", "ìš”ì•½", "MCQ(ê°ê´€ì‹)", "OX(ì°¸/ê±°ì§“)", "ë‹¨ë‹µí˜• í€´ì¦ˆ", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"],
        index=0
    )
    if response_mode in ["MCQ(ê°ê´€ì‹)", "OX(ì°¸/ê±°ì§“)", "ë‹¨ë‹µí˜• í€´ì¦ˆ", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
        num_q = st.slider("ë¬¸í•­ ìˆ˜", 1, 15, 5, 1)
    else:
        num_q = 0
    if response_mode in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
        num_choices = st.slider("ë³´ê¸° ê°œìˆ˜(ê°ê´€ì‹)", 2, 6, 4, 1)
    else:
        num_choices = 0

    st.markdown("### ğŸ“„ íŒŒì¼ ì…ë ¥")
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=['pdf', 'txt'])
    use_semantic_chunk_txt = st.checkbox("TXTì— ì‹œë§¨í‹± ì²­í‚¹ ì‚¬ìš©(ë¹„ìš©â†‘)", value=False)
    chunk_size = st.number_input("ì²­í¬ í¬ê¸°", min_value=200, max_value=4000, value=1000, step=50)
    chunk_overlap = st.number_input("ì²­í¬ ì˜¤ë²„ë©", min_value=0, max_value=1000, value=100, step=10)

# =========================
# 7) ë¡œì»¬ ìºì‹œ ê²½ë¡œ
# =========================
os.makedirs("./mycache/files", exist_ok=True)
os.makedirs("./mycache/embedding", exist_ok=True)
store = LocalFileStore("./mycache/embedding")

# =========================
# 8) RAG êµ¬ì„± ìš”ì†Œ
# =========================
class FallbackRetriever(Runnable):
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback
    def invoke(self, input, config=None):
        try:
            results = self.primary.invoke(input, config=config)
            if results:
                return results
        except Exception:
            pass
        return self.fallback.invoke(input, config=config)

join_docs = RunnableLambda(lambda docs: "\n".join(getattr(d, "page_content", str(d)) for d in (docs or [])))

def _openai_embeddings():
    return OpenAIEmbeddings(model="text-embedding-3-small", api_key=os.getenv("OPENAI_API_KEY"))

def make_vectorstore_from_documents(split_documents):
    cached = CacheBackedEmbeddings.from_bytes_store(_openai_embeddings(), store)
    return FAISS.from_documents(documents=split_documents, embedding=cached)

@st.cache_resource(show_spinner=True)
def process_pdf_to_retriever(file_path: str, chunk_size: int, chunk_overlap: int):
    docs = PDFPlumberLoader(file_path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    vectorstore = make_vectorstore_from_documents(splitter.split_documents(docs))
    return vectorstore.as_retriever()

@st.cache_resource(show_spinner=True)
def process_txt_to_retriever(text: str, use_semantic: bool, chunk_size: int, chunk_overlap: int):
    if use_semantic:
        splitter = SemanticChunker(_openai_embeddings(), breakpoint_threshold_type="percentile", breakpoint_threshold_amount=70)
        docs = splitter.create_documents([text])
    else:
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        docs = splitter.create_documents([text])
    vectorstore = make_vectorstore_from_documents(docs)
    return vectorstore.as_retriever()

def _safe_tavily_retriever(k: int = 3):
    if TAVILY_API_KEY:
        os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
        return TavilySearchAPIRetriever(k=k)
    else:
        class EmptyRetriever(Runnable):
            def invoke(self, input, config=None): return []
        st.sidebar.warning("Tavily í‚¤ê°€ ì—†ì–´ ì›¹ í´ë°± ê²€ìƒ‰ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        return EmptyRetriever()

# =========================
# 9) í”„ë¡¬í”„íŠ¸ (history/active_context ë°˜ì˜ + íƒœê·¸/ë¼ë²¨ ìš°ì„ )
# =========================
def build_prompt_text(mode: str) -> str:
    header_common = """[ì§€ì‹œ]
- í˜„ì¬ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ê³¼ê±° íˆìŠ¤í† ë¦¬ëŠ” **ë¬´ì‹œ**í•˜ë¼(ìë™ ì£¼ì œ ì „í™˜ ì‹œ ì´ì „ ì£¼ì œëŠ” ì œì™¸ë¨).
- ì•„ë˜ì˜ <chat_history>ì™€ <active_context>ë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ë¼.
- ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì¼ë°˜ëª…ì‚¬ ìœ„ì£¼ì¼ ê²½ìš°, ì‚¬ìš©ìê°€ ë²”ìœ„ë¥¼ ë°”ê¾¸ì—ˆë‹¤ê³  **ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´**
  <active_context>ì˜ ë²”ìœ„ë¥¼ **ê¸°ë³¸ê°’**ìœ¼ë¡œ ê°€ì •í•˜ë¼.
- ì§ˆë¬¸ ë¬¸ìì—´ì— **[í™œì„±ì£¼ì œ: â€¦]** íƒœê·¸ë‚˜ **'â€¦ ê´€ë ¨' ë„ë©”ì¸ ë¼ë²¨**ì´ ë³´ì´ë©´, ê·¸ ë²”ìœ„ë¥¼ **ìµœìš°ì„ ìœ¼ë¡œ ì ìš©**í•˜ë¼.
"""

    history_block = """
<chat_history>
{history}
</chat_history>

<active_context>
{active_context}
</active_context>
"""

    if mode == "ì¼ë°˜ ëŒ€í™”":
        return f"""ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

{header_common}
- ì•„ë˜ì˜ <information> ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  í™œìš©í•˜ë˜, ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë¼.
- ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ë¼.
{history_block}
<information>
{{context}}
</information>

#Question:
{{question}}
"""

    if mode == "ìš”ì•½":
        return f"""ë‹¤ìŒ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. í•„ìš”ì‹œ í•­ëª©ë³„ ë¶ˆë¦¿ì„ ì‚¬ìš©í•˜ë¼.
{header_common}
{history_block}
<information>
{{context}}
</information>

ìš”ì•½ ëŒ€ìƒ/ìš”ì²­:
{{question}}
"""

    if mode == "MCQ(ê°ê´€ì‹)":
        return f"""ë‹¹ì‹ ì€ ê°ê´€ì‹ ë¬¸í•­ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ê°ê´€ì‹ ë¬¸í•­ì„ ë§Œë“¤ì–´ë¼.
- ê° ë¬¸í•­ì€ ë³´ê¸° {{num_choices}}ê°œë¡œ êµ¬ì„±í•˜ê³ , ë³´ê¸° í‘œì‹œëŠ” ë‹¤ìŒ ë¼ë²¨ì„ **ì´ ìˆœì„œ**ë¡œ ì‚¬ìš©í•˜ë¼: {{choice_labels_text}}
- ë¬¸ì œë¬¸ ë‹¤ìŒ ì¤„ë¶€í„° ê° ë³´ê¸°ë¥¼ **í•œ ì¤„ì— í•˜ë‚˜ì”©** ì¶œë ¥í•˜ë¼.
- ë§ˆì§€ë§‰ ì¤„ì— ì •ë‹µì„ 'Answer: â‘ ' í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ(í˜•ì‹ë§Œ ì°¸ê³ )
Question 1. â€¦ë¬¸ì œë¬¸â€¦
â‘  ë³´ê¸°1
â‘¡ ë³´ê¸°2
â‘¢ ë³´ê¸°3
â‘£ ë³´ê¸°4
Answer: â‘¢
"""

    if mode == "OX(ì°¸/ê±°ì§“)":
        return f"""ë‹¹ì‹ ì€ OX(ì°¸/ê±°ì§“) ë¬¸ì œ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ì§„ìˆ ë¬¸ì„ ë§Œë“¤ê³ , ê° ë¬¸í•­ì˜ ì •ë‹µì„ 'Answer: O' ë˜ëŠ” 'Answer: X' í•œ ì¤„ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ
Q1. â€¦ì§„ìˆ ë¬¸â€¦
Answer: O

Q2. â€¦ì§„ìˆ ë¬¸â€¦
Answer: X
"""

    if mode == "ë‹¨ë‹µí˜• í€´ì¦ˆ":
        return f"""ë‹¹ì‹ ì€ ë‹¨ë‹µí˜• í€´ì¦ˆ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œì˜ ë‹¨ë‹µí˜• ë¬¸í•­ì„ ë§Œë“¤ê³ , ê° ë¬¸í•­ì˜ ì •ë‹µì„ 'Answer: â€¦' í•œ ì¤„ë¡œ ëª…ì‹œí•˜ë¼.
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}

#ì˜ˆì‹œ
Q1. â€¦ë¬¸ì œë¬¸â€¦
Answer: â€¦

Q2. â€¦ë¬¸ì œë¬¸â€¦
Answer: â€¦
"""

    # í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)
    return f"""ë‹¹ì‹ ì€ í˜¼í•©í˜• í€´ì¦ˆ ìƒì„±ê¸°ì…ë‹ˆë‹¤.

{header_common}
- ì´ {{num_q}}ê°œ ë¬¸í•­ì„ ìƒì„±í•˜ë˜, ëŒ€ëµ 50%ëŠ” ê°ê´€ì‹, 25%ëŠ” OX, 25%ëŠ” ë‹¨ë‹µí˜•ìœ¼ë¡œ ì„ì–´ë¼.
- ê°ê´€ì‹ì˜ ë³´ê¸° ìˆ˜ëŠ” {{num_choices}}ê°œì´ë©°, ë³´ê¸° ë¼ë²¨ì€ ë‹¤ìŒì„ ì‚¬ìš©: {{choice_labels_text}}
- ê° ìœ í˜•ì˜ ì¶œë ¥ í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œë¼:
  - ê°ê´€ì‹: ë¬¸ì œë¬¸ -> ê° ë³´ê¸°(í•œ ì¤„ì”©) -> 'Answer: â‘ ' í˜•íƒœ
  - OX: 'Qn. â€¦' -> 'Answer: O/X'
  - ë‹¨ë‹µí˜•: 'Qn. â€¦' -> 'Answer: â€¦'
- ë¬¸í•­ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë‘”ë‹¤.
{history_block}
<information>
{{context}}
</information>

#Question(ì£¼ì œ/ìš”ì²­):
{{question}}
"""

# =========================
# 10) ì²´ì¸
# =========================
def create_chain(retriever, mode: str, num_q: int, num_choices: int, choice_labels_text: str):
    tavily = _safe_tavily_retriever(k=3)
    fallback = FallbackRetriever(retriever, tavily)
    prompt = ChatPromptTemplate.from_template(build_prompt_text(mode))
    llm = ChatOpenAI(model="gpt-4.1", temperature=0, api_key=os.getenv("OPENAI_API_KEY"))
    return (
        RunnableMap({
            "context": fallback | join_docs,
            "question": RunnablePassthrough(),
            "history": RunnableLambda(lambda _: memory.render_capped(MAX_HISTORY_CHARS) if use_memory else ""),
            "active_context": RunnableLambda(lambda _: memory.scope_hint()),
            "num_q": RunnableLambda(lambda _: num_q),
            "num_choices": RunnableLambda(lambda _: num_choices),
            "choice_labels_text": RunnableLambda(lambda _: choice_labels_text),
        })
        | prompt
        | llm
        | StrOutputParser()
    )

# =========================
# 11) ì—…ë¡œë“œ ì²˜ë¦¬ & ë¦¬íŠ¸ë¦¬ë²„
# =========================
retriever = None
if uploaded_file:
    ext = uploaded_file.name.split(".")[-1].lower()
    save_path = os.path.join("./mycache/files", f"{uuid.uuid4().hex}.{ext}")
    try:
        file_bytes = uploaded_file.read()
        if ext == "pdf":
            with open(save_path, "wb") as fw: fw.write(file_bytes)
            retriever = process_pdf_to_retriever(save_path, int(chunk_size), int(chunk_overlap))
        elif ext == "txt":
            text = file_bytes.decode("utf-8", errors="ignore")
            with open(save_path, "w", encoding="utf-8") as fw: fw.write(text)
            retriever = process_txt_to_retriever(text, use_semantic_chunk_txt, int(chunk_size), int(chunk_overlap))
        else:
            st.error("pdf/txtë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

if retriever is None:
    retriever = _safe_tavily_retriever(k=3)

if not OPENAI_API_KEY:
    st.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /home/mk/workspace/KEYS.env í™•ì¸.")
    st.stop()

# =========================
# 12) ì±„íŒ… ì²˜ë¦¬
# =========================
user_input = st.chat_input("ì›í•˜ëŠ” ëª¨ë“œì™€ í•¨ê»˜ ì•„ë¬´ ì£¼ì œë‚˜ ì§ˆë¬¸í•˜ì„¸ìš”. (ì˜ˆ: 'SQLD í•µì‹¬ ìš”ì•½', 'ê°ê´€ì‹ 5ë¬¸ì œ', 'ì•¼êµ¬ ê·œì¹™ OX' ë“±)")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    if is_meta_memory_query(user_input):
        prev_q = memory.last_user_text()
        reply = f"ë‹¹ì‹ ì´ ì „ì— ë¬¼ì–´ë³¸ ê²ƒì€ ë‹¤ìŒì…ë‹ˆë‹¤:\n\n> {prev_q}" if prev_q else "ì§ì „ì— ì €ì¥ëœ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì—†ì–´ìš”."
        with st.chat_message('assistant'):
            st.markdown(f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{reply}")
        st.session_state.messages.append({"role": "assistant", "content": f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{reply}"})
        memory.add_turn(user_input, reply)
        st.stop()

    if use_memory and auto_topic:
        try:
            if memory.is_new_topic(user_input, threshold=topic_threshold):
                memory.reset()
                st.toast("ğŸ”„ ìƒˆ ì£¼ì œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ”")
        except Exception as e:
            st.sidebar.warning(f"ì£¼ì œ ì „í™˜ ê°ì§€ ê²½ê³ : {e}")

    if topic_debug and "__topic_debug" in st.session_state:
        sim, thr = st.session_state["__topic_debug"]
        st.sidebar.info(f"ìœ ì‚¬ë„: {sim:.3f} / ì ìš© ì„ê³„ê°’: {thr:.2f}")

    choice_labels = get_choice_labels(num_choices) if num_choices >= 2 else []
    choice_labels_text = " ".join(choice_labels) if choice_labels else ""
    chain = create_chain(retriever, response_mode, num_q, num_choices, choice_labels_text)

    # â˜… í™œì„± ë§¥ë½ + ë„ë©”ì¸ ë¼ë²¨ì„ ë°˜ì˜í•œ 'ì‹¤ì œ ì§ˆì˜'
    scope_hint = memory.scope_hint()
    topic_lbl = memory.topic_label()
    effective_query = rewrite_with_active_context(user_input, scope_hint, topic_lbl)
    if context_debug:
        st.sidebar.caption(f"scope_hint: {scope_hint}")
        st.sidebar.caption(f"topic_label: {topic_lbl}")
        st.sidebar.caption(f"effective_query: {effective_query}")

    try:
        response_stream = chain.stream(effective_query)
        with st.chat_message('assistant'):
            container = st.empty()
            ai_answer = ""
            for token in response_stream:
                ai_answer += token
                preview = ai_answer if response_mode not in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"] else to_hard_breaks(format_mcq(ai_answer))
                container.markdown(f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{preview}")

            if response_mode in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
                final_text = to_hard_breaks(format_mcq(ai_answer))
            else:
                final_text = ai_answer

            final_render = f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{final_text}"
            container.markdown(final_render)

        st.session_state.messages.append({"role": "assistant", "content": final_render})
        memory.add_turn(user_input, final_text)

    except Exception as e:
        err = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        st.chat_message("assistant").error(err)
        st.session_state.messages.append({"role": "assistant", "content": err})
