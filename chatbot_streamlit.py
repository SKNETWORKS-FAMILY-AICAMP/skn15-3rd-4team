# app.py
# Streamlit UI that uses ChatBotEngine

import os, uuid
import streamlit as st

from chatbot_engine_mk_not_yet import (
    ChatBotEngine,
    format_mcq, to_hard_breaks, get_choice_labels,
)

# ---------- Page / Session ----------
st.set_page_config(page_title="RAG ê¸°ë°˜ ë©€í‹°ëª¨ë“œ ì±—ë´‡", layout="wide")
st.title("RAG ê¸°ë°˜ ë©€í‹°ëª¨ë“œ ì±—ë´‡")

if "session_id" not in st.session_state:
    st.session_state.session_id = uuid.uuid4().hex
if "messages" not in st.session_state:
    st.session_state.messages = []
if "engine" not in st.session_state:
    st.session_state.engine = ChatBotEngine(session_id=st.session_state.session_id)
ENGINE: ChatBotEngine = st.session_state.engine

# ---------- Chat log (scrollable) ----------
st.markdown('<div id="chatlog" style="max-height:72vh; overflow-y:auto; padding-right:8px;">', unsafe_allow_html=True)
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
st.markdown("</div>", unsafe_allow_html=True)

# ---------- Sidebar ----------
with st.sidebar:
    def mask(s): return (s[:4] + "..." + s[-4:]) if s else "None"
    st.markdown("### ğŸ”‘ Keys")
    st.write("OpenAI:", bool(os.getenv("OPENAI_API_KEY")), mask(os.getenv("OPENAI_API_KEY") or ""))
    st.write("Tavily:", bool(os.getenv("TAVILY_API_KEY")), mask(os.getenv("TAVILY_API_KEY") or ""))

    st.markdown("### ğŸ§  Memory")
    use_memory = st.checkbox("ì´ì „ ëŒ€í™” ê¸°ì–µ ì‚¬ìš©", value=True)
    auto_topic = st.checkbox("ìë™ ì£¼ì œ ì „í™˜ ê°ì§€", value=True)
    topic_threshold = st.slider("ì£¼ì œ ì „í™˜ ì„ê³„ê°’(ìœ ì‚¬ë„)", 0.00, 1.00, 0.72, 0.01)
    topic_debug = st.checkbox("í† í”½ ë””ë²„ê·¸ í‘œì‹œ", value=False)
    context_debug = st.checkbox("ë§¥ë½ ë””ë²„ê·¸ í‘œì‹œ", value=False)

    if st.button("ë©”ëª¨ë¦¬ ìˆ˜ë™ ì´ˆê¸°í™”"):
        ENGINE.reset_topic()
        st.success("ë©”ëª¨ë¦¬ì™€ ë¼ë²¨ ìºì‹œë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
    if st.button("ğŸ§¹ í™”ë©´ ì±„íŒ… ë¡œê·¸ ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.success("í™”ë©´ ì±„íŒ… ë¡œê·¸ë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤. (ì—”ì§„ ë©”ëª¨ë¦¬ëŠ” ê·¸ëŒ€ë¡œ)")

    st.markdown("### ğŸ§© ì‘ë‹µ ëª¨ë“œ")
    response_mode = st.selectbox(
        "ì›í•˜ëŠ” ì¶œë ¥ í˜•íƒœ",
        ["ì¼ë°˜ ëŒ€í™”", "ìš”ì•½", "MCQ(ê°ê´€ì‹)", "OX(ì°¸/ê±°ì§“)", "ë‹¨ë‹µí˜• í€´ì¦ˆ", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"],
        index=0
    )
    if response_mode in ["MCQ(ê°ê´€ì‹)", "OX(ì°¸/ê±°ì§“)", "ë‹¨ë‹µí˜• í€´ì¦ˆ", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
        num_q = st.slider("ë¬¸í•­ ìˆ˜", 1, 15, 5, 1)
    else:
        num_q = 0
    if response_mode in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
        num_choices = st.slider("ë³´ê¸° ê°œìˆ˜(ê°ê´€ì‹)", 2, 6, 4, 1)
    else:
        num_choices = 0

    st.markdown("### ğŸ“„ íŒŒì¼ ì…ë ¥")
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "txt"])
    use_semantic_chunk_txt = st.checkbox("TXTì— ì‹œë§¨í‹± ì²­í‚¹ ì‚¬ìš©(ë¹„ìš©â†‘)", value=False)
    chunk_size = st.number_input("ì²­í¬ í¬ê¸°", min_value=200, max_value=4000, value=1000, step=50)
    chunk_overlap = st.number_input("ì²­í¬ ì˜¤ë²„ë©", min_value=0, max_value=1000, value=100, step=10)

# ---------- Retriever ----------
retriever = None
if uploaded_file:
    import os
    ext = uploaded_file.name.split(".")[-1].lower()
    os.makedirs("./mycache/files", exist_ok=True)
    save_path = os.path.join("./mycache/files", f"{uuid.uuid4().hex}.{ext}")
    try:
        file_bytes = uploaded_file.read()
        if ext == "pdf":
            with open(save_path, "wb") as fw: fw.write(file_bytes)
            retriever = ENGINE.build_retriever_from_pdf(save_path, int(chunk_size), int(chunk_overlap))
        elif ext == "txt":
            text = file_bytes.decode("utf-8", errors="ignore")
            with open(save_path, "w", encoding="utf-8") as fw: fw.write(text)
            retriever = ENGINE.build_retriever_from_text(text, use_semantic_chunk_txt, int(chunk_size), int(chunk_overlap))
        else:
            st.error("pdf/txtë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

if retriever is None:
    retriever = ENGINE.safe_tavily_retriever(k=3)

if not os.getenv("OPENAI_API_KEY"):
    st.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /home/mk/workspace/KEYS.env ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# ---------- Chat Input ----------
user_input = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”. (ì˜ˆ: 'SQLD ê³µë¶€ë²•', 'ê°ê´€ì‹ 5ë¬¸ì œ', 'ì•¼êµ¬ ê·œì¹™ OX' ë“±)")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    # 1) ë©”íƒ€ ë©”ëª¨ë¦¬ ì§ˆë¬¸ì€ ì¦‰ì‹œ ì²˜ë¦¬
    if ENGINE.is_meta_memory_query(user_input):
        prev_q = ENGINE.memory.last_user_text()
        reply = f"ë‹¹ì‹ ì´ ì „ì— ë¬¼ì–´ë³¸ ê²ƒì€ ë‹¤ìŒì…ë‹ˆë‹¤:\n\n> {prev_q}" if prev_q else "ì§ì „ì— ì €ì¥ëœ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì—†ì–´ìš”."
        render = f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{reply}"
        with st.chat_message("assistant"):
            st.markdown(render)
        st.session_state.messages.append({"role": "assistant", "content": render})
        ENGINE.memory.add_turn(user_input, reply)
        st.stop()

    # 2) ìë™ ì£¼ì œ ì „í™˜ (ì§€ì‹œí‘œí˜„ í•˜ë“œê°€ë“œ í¬í•¨)
    if use_memory and auto_topic:
        try:
            reset, dbg = ENGINE.should_reset_topic(user_input, threshold=topic_threshold, sticky_margin=0.05)
            if topic_debug and dbg:
                sim, thr = dbg
                st.sidebar.info(f"ìœ ì‚¬ë„: {sim:.3f} / ì ìš© ì„ê³„ê°’: {thr:.2f}")
            if reset:
                ENGINE.reset_topic()
                st.toast("ğŸ”„ ìƒˆ ì£¼ì œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ”")
        except Exception as e:
            st.sidebar.warning(f"ì£¼ì œ ì „í™˜ ê°ì§€ ê²½ê³ : {e}")

    # 3) ì²´ì¸ ìƒì„± & ì‹¤ì œ ì§ˆì˜(í™œì„±ë§¥ë½ + ë„ë©”ì¸ ë¼ë²¨ ì£¼ì…)
    choice_labels = get_choice_labels(num_choices) if num_choices >= 2 else []
    choice_labels_text = " ".join(choice_labels) if choice_labels else ""
    chain = ENGINE.create_chain(retriever, response_mode, num_q, num_choices, choice_labels_text)

    scope_hint = ENGINE.scope_hint()
    topic_lbl = ENGINE.topic_label()  # â† ìºì‹œ í¬í•¨
    effective_query = ENGINE.rewrite_with_active_context(user_input, scope_hint, topic_lbl)
    if context_debug:
        st.sidebar.caption(f"scope_hint: {scope_hint}")
        st.sidebar.caption(f"topic_label: {topic_lbl or '(ì—†ìŒ)'}")
        st.sidebar.caption(f"effective_query: {effective_query}")

    # 4) ì‹¤í–‰ + ë Œë”
    try:
        response_stream = chain.stream(effective_query)
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response_stream:
                ai_answer += token
                preview = ai_answer
                if response_mode in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
                    preview = to_hard_breaks(format_mcq(preview))
                container.markdown(f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{preview}")

            final_text = ai_answer
            if response_mode in ["MCQ(ê°ê´€ì‹)", "í˜¼í•©(ê°ê´€+OX+ë‹¨ë‹µ)"]:
                final_text = to_hard_breaks(format_mcq(final_text))
            final_render = f"**ğŸ§‘ ì§ˆë¬¸:** {user_input}\n\n{final_text}"
            container.markdown(final_render)

        st.session_state.messages.append({"role": "assistant", "content": final_render})
        ENGINE.memory.add_turn(user_input, final_text)

    except Exception as e:
        err = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        with st.chat_message("assistant"):
            st.error(err)
        st.session_state.messages.append({"role": "assistant", "content": err})
