# tools.py
import json
import requests
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PDFPlumberLoader, Docx2txtLoader
from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
import time
import chardet
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

ANKI_CONNECT_URL = "http://192.168.160.1:8765"
tavily_search_tool = TavilySearchResults(max_results=3)

# ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
VECTORSTORE_DSN = "postgresql+psycopg2://play:123@localhost:5432/play"
DOCUMENT_COLLECTION_NAME = "play_documents"
EPHEMERAL_STORES = {}  # key: file_name, value: FAISS VectorStore

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
document_vector_store = PGVector(
    connection=VECTORSTORE_DSN,
    embeddings=embeddings,
    collection_name=DOCUMENT_COLLECTION_NAME,
    use_jsonb=True,
)

def web_search(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ ë„êµ¬"""
    try:
        print(f"ğŸ–¥ï¸  ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query}")
        results = tavily_search_tool.invoke({"query": query})
        return f"'{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n{results}"
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def document_search(query: str) -> str:
    """
    ì—…ë¡œë“œëœ ì¸ë©”ëª¨ë¦¬ FAISS ì¸ë±ìŠ¤(EPHEMERAL_STORES)ì—ì„œë§Œ ê²€ìƒ‰.
    - DBëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    - ê¸°ì¡´ì˜ ìƒì„¸ ë””ë²„ê¹…/ì ìˆ˜ ê³„ì‚° ë¡œì§ì€ ìœ ì§€Â·ê°„ì†Œí™”
    """
    try:
        print(f"ğŸ“„ ì¸ë©”ëª¨ë¦¬ ë¬¸ì„œ ê²€ìƒ‰: '{query}'")

        if not EPHEMERAL_STORES:
            return "ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

        # ê° íŒŒì¼ ì¸ë±ìŠ¤ì—ì„œ kê°œì”© ìˆ˜ì§‘
        k_per_file = 5
        gathered = []
        for fname, store in EPHEMERAL_STORES.items():
            try:
                docs = store.as_retriever(search_kwargs={"k": k_per_file}).get_relevant_documents(query)
                gathered.extend(docs)
            except Exception as e:
                print(f"   âŒ '{fname}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        if not gathered:
            return f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        # ì ìˆ˜ ê³„ì‚°(ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        q_emb = embeddings.embed_query(query)
        scored = []
        for i, d in enumerate(gathered):
            try:
                d_emb = embeddings.embed_query(d.page_content)
                sim = cosine_similarity([q_emb], [d_emb])[0][0]
                scored.append({
                    "document": d,
                    "score": float(sim),
                    "source": d.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "chunk_id": d.metadata.get("chunk_id", i),
                })
            except Exception as e:
                print(f"   âŒ ì ìˆ˜ê³„ì‚° ì‹¤íŒ¨: {e}")

        if not scored:
            return "ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        scored.sort(key=lambda x: x['score'], reverse=True)

        # ì„ê³„ê°’(ê´€ëŒ€)
        PRIMARY = 0.3
        FALLBACK = 0.1
        picked = [x for x in scored if x['score'] >= PRIMARY] or \
                 [x for x in scored if x['score'] >= FALLBACK] or \
                 scored[:5]

        # ì¤‘ë³µ ì œê±°(ê°„ë‹¨)
        seen = set()
        unique = []
        for x in picked:
            key = (x['source'], str(x['document'].page_content)[:100].lower())
            if key not in seen:
                seen.add(key)
                unique.append(x)

        top = unique[:5]
        if not top:
            return f"'{query}'ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        parts = []
        scores = [x['score'] for x in scored]
        for i, x in enumerate(top, 1):
            lvl = "ë†’ìŒ" if x['score'] >= 0.5 else "ì¤‘ê°„" if x['score'] >= 0.3 else "ë‚®ìŒ"
            content = x['document'].page_content.strip()
            parts.append(
                f"ğŸ“‹ **ë¬¸ì„œ {i}** (ê´€ë ¨ì„±: {lvl}) - {x['source']} (ì²­í¬ {x['chunk_id']})\n{content}\n"
            )

        return (
            f"ğŸ” **'{query}' ê²€ìƒ‰ ê²°ê³¼** ({len(parts)}ê°œ ê´€ë ¨ ë¬¸ì„œ)\n\n" +
            "\n".join(parts) +
            f"\nğŸ“Œ **ê²€ìƒ‰ ì •ë³´**: í›„ë³´ {len(scored)}ê°œ ì¤‘ ìƒìœ„ {len(top)}ê°œ ì„ ë³„\n" +
            f"ğŸ“Š **ì ìˆ˜ ë²”ìœ„**: {max(scores):.3f} ~ {min(scores):.3f} (í‰ê·  {sum(scores)/len(scores):.3f})"
        )

    except Exception as e:
        return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# ğŸ”¥ ê°„ì†Œí™”ëœ ì¤‘ë³µ ì œê±° í•¨ìˆ˜
def remove_duplicate_content(scored_docs: list) -> list:
    """ì¤‘ë³µ ì œê±°ë¥¼ ë” ê°„ë‹¨í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬"""
    if len(scored_docs) <= 1:
        return scored_docs
    
    unique_docs = []
    seen_contents = set()
    
    for doc_info in scored_docs:
        content = doc_info['document'].page_content
        # ë‚´ìš©ì˜ ì²« 100ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ë” ê°„ë‹¨í•œ ë°©ì‹)
        content_key = content[:100].strip().lower()
        
        if content_key not in seen_contents:
            unique_docs.append(doc_info)
            seen_contents.add(content_key)
    
    return unique_docs

# ğŸ”¥ ì¶”ê°€: ë¬¸ì„œ í˜„í™© í™•ì¸ í•¨ìˆ˜
def check_document_status() -> str:
    """ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™©ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰
        retriever = document_vector_store.as_retriever(search_kwargs={"k": 100})
        all_docs = retriever.invoke("*")
        
        if not all_docs:
            return "âŒ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¬¸ì„œë³„ í†µê³„
        sources = {}
        for doc in all_docs:
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            if source not in sources:
                sources[source] = 0
            sources[source] += 1
        
        status_msg = f"ğŸ“Š **ì—…ë¡œë“œëœ ë¬¸ì„œ í˜„í™©** (ì´ {len(all_docs)}ê°œ ì²­í¬)\n\n"
        
        for source, count in sources.items():
            status_msg += f"ğŸ“„ {source}: {count}ê°œ ì²­í¬\n"
        
        # ìƒ˜í”Œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        status_msg += f"\n**ìƒ˜í”Œ ë‚´ìš©:**\n"
        for i, doc in enumerate(all_docs[:3], 1):
            preview = doc.page_content[:50].replace('\n', ' ')
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            status_msg += f"{i}. [{source}] {preview}...\n"
        
        return status_msg
        
    except Exception as e:
        return f"âŒ ë¬¸ì„œ í˜„í™© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"

def jaccard_similarity(text1: str, text2: str) -> float:
    """
    ë‘ í…ìŠ¤íŠ¸ì˜ Jaccard ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    # Jaccard similarity = |A âˆ© B| / |A âˆª B|
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    if not union:
        return 0.0
    
    return len(intersection) / len(union)

def upload_document(file_path: str, file_name: str) -> str:
    """
    ğŸ”¥ ì˜êµ¬ DBì— ì €ì¥í•˜ì§€ ì•Šê³ , í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬(ëª¨ë“ˆ ì „ì—­)ì—ë§Œ ì¸ë±ìŠ¤ ìƒì„±
    - PDF/DocxëŠ” ë¡œë”ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - txt/mdëŠ” ê·¸ëŒ€ë¡œ ë””ì½”ë”©
    - FAISS ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ ìƒì„± â†’ EPHEMERAL_STORES[file_name]ì— ì €ì¥
    """
    try:
        print(f"ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ(ì¸ë©”ëª¨ë¦¬): {file_name}")

        ext = (file_name.rsplit(".", 1)[-1] if "." in file_name else "").lower()
        texts = []

        if ext == "pdf":
            loader = PDFPlumberLoader(file_path)
            docs = loader.load()
            texts = [d.page_content for d in docs if d.page_content and d.page_content.strip()]
        elif ext in ("docx", "doc"):
            loader = Docx2txtLoader(file_path)
            docs = loader.load()
            texts = [d.page_content for d in docs if d.page_content and d.page_content.strip()]
        else:
            # txt, md ë“± ì¼ë°˜ í…ìŠ¤íŠ¸
            with open(file_path, 'rb') as f:
                raw = f.read()
            import chardet
            enc = chardet.detect(raw)['encoding'] or 'utf-8'
            content = raw.decode(enc, errors='replace').replace('\x00', '')
            texts = [content]

        if not texts:
            return f"âŒ '{file_name}'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        # ê¸°ì¡´ê³¼ ë™ì¼í•œ ë¶„í•  ì •ì±…(ì •í™•ë„ â†‘)
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=150,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        chunks = []
        for t in texts:
            chunks.extend(splitter.split_text(t))

        if not chunks:
            return f"âŒ '{file_name}'ì—ì„œ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤."

        metadatas = [{"source": file_name, "chunk_id": i} for i in range(len(chunks))]

        # ğŸ”¥ ì¸ë©”ëª¨ë¦¬ FAISS ì¸ë±ìŠ¤ ìƒì„± (DBì— ì“°ì§€ ì•ŠìŒ)
        vs = FAISS.from_texts(texts=chunks, embedding=embeddings, metadatas=metadatas)

        # ì „ì—­ ì €ì¥(ì„¸ì…˜/í”„ë¡œì„¸ìŠ¤ í•œì •)
        EPHEMERAL_STORES[file_name] = vs

        return f"âœ… **'{file_name}' ì—…ë¡œë“œ ì™„ë£Œ!** (ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤, ì²­í¬ {len(chunks)}ê°œ)"

    except Exception as e:
        return f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


def anki_card_saver(front: str, back: str, deck: str = "ê¸°ë³¸", tags: list = None) -> str:
    """
    AnkiConnect APIë¥¼ ì‚¬ìš©í•˜ì—¬ Anki ë°ìŠ¤í¬í†± í”„ë¡œê·¸ë¨ì— ìƒˆ ì¹´ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print(f"ğŸƒ Anki ì¹´ë“œ ì €ì¥ ì‹œë„: {front[:30]}...")
    
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}

    try:
        # 1. ì¤‘ë³µ ì¹´ë“œ í™•ì¸ - ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ ì•ë©´ ë‚´ìš©ì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        front_keywords = front.replace('\n', ' ')[:50]  # ì•ë©´ì˜ ì²« 50ìë¡œ ì¤‘ë³µ ê²€ì‚¬
        
        query = f'deck:"{deck}" front:*{front_keywords}*'
        find_payload = anki_request('findNotes', query=query)
        response = requests.post(ANKI_CONNECT_URL, json=find_payload)
        response.raise_for_status()
        
        print(f"   -> Anki 'findNotes' ì‘ë‹µ: {response.json()}")
        
        existing_notes = response.json().get('result', [])
        if existing_notes:
            message = f"ìœ ì‚¬í•œ ì¹´ë“œê°€ '{deck}' ë±ì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            print(f"   -> {message}")
            return message

        # 2. ìƒˆ ë…¸íŠ¸(ì¹´ë“œ) ì¶”ê°€
        note_params = {
            'note': {
                'deckName': deck,
                'modelName': 'Basic',
                'fields': {
                    'Front': front,
                    'Back': back.replace("\n", "<br>")  # HTML ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                },
                'tags': tags if tags else []
            }
        }
        
        add_payload = anki_request('addNote', **note_params)
        response = requests.post(ANKI_CONNECT_URL, json=add_payload)
        response.raise_for_status()

        print(f"   -> Anki 'addNote' ì‘ë‹µ: {response.json()}")

        response_data = response.json()
        if error := response_data.get('error'):
            raise Exception(f"AnkiConnect ì˜¤ë¥˜: {error}")
        
        note_id = response_data.get('result')
        message = f"âœ… Anki ì¹´ë“œë¥¼ '{deck}' ë±ì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (ID: {note_id})"
        print(f"   -> {message}")
        return message

    except requests.exceptions.ConnectionError:
        error_msg = "âŒ AnkiConnectì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Anki í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ ì¤‘ì´ê³  AnkiConnect ì• ë“œì˜¨ì´ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        print(f"   -> {error_msg}")
        return error_msg
    except Exception as e:
        error_msg = f"âŒ Anki ì¹´ë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"   -> {error_msg}")
        return error_msg

def list_anki_decks() -> str:
    """Ankiì˜ ëª¨ë“  ë± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"""
    def anki_request(action, **params):
        return {'action': action, 'version': 6, 'params': params}
    
    try:
        payload = anki_request('deckNames')
        response = requests.post(ANKI_CONNECT_URL, json=payload)
        response.raise_for_status()
        
        decks = response.json().get('result', [])
        if decks:
            return f"ì‚¬ìš© ê°€ëŠ¥í•œ Anki ë± ëª©ë¡:\n" + "\n".join([f"- {deck}" for deck in decks])
        else:
            return "ì‚¬ìš© ê°€ëŠ¥í•œ ë±ì´ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë± ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"